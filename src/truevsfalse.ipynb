{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Projet Data Science </h1></div>\n",
    "<div align=\"center\"><h2> Classification d'assertions selon leur valeurs de véracité ( automatic fact-checking ) </h2></div>\n",
    "<h2>Membre du groupe</h2>\n",
    "<ul>\n",
    "    <li>Meriem AMERAOUI</li>\n",
    "    <li>Dounia BELABIOD</li>\n",
    "    <li>Jihene BOUHLEL</li>\n",
    "    <li>Bahaa Eddine NIL</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Executing the basic\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import inflect\n",
    "import re\n",
    "import nltk\n",
    "#import contractions\n",
    "\n",
    "#from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Cleaning the text\n",
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word) \n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def separate_letter_number(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        nw = re.findall('\\d+|\\D+', word)\n",
    "        new_words.append(nw)\n",
    "    new_words = reduce(lambda x,y: x+y,new_words)\n",
    "    return new_words\n",
    "\n",
    "def replace_contractions(text):\n",
    "    return \n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    #words = separate_letter_number(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def clean_text(text):\n",
    "    porterStemmer = PorterStemmer()\n",
    "    lancasterStemmer = LancasterStemmer()\n",
    "    wordNetLemmatizer = WordNetLemmatizer()\n",
    "    #tokens = word_tokenize(contractions.fix(text))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = normalize(tokens)\n",
    "    #tokens = [porterStemmer.stem(word) for word in tokens]\n",
    "    #tokens = [lancasterStemmer.stem(word) for word in tokens]\n",
    "    #tokens = [wordNetLemmatizer.lemmatize(word, pos = 'v') for word in tokens]\n",
    "    text=\"\".join([\" \"+i for i in tokens]).strip()\n",
    "    return text\n",
    "\n",
    "# Up-sampling & Down-sampling\n",
    "def sampling(df_func, *args, **kwargs):\n",
    "    sampling_type = kwargs.get(\"sample\", None)\n",
    "    if(sampling_type not in ['up', 'down']):\n",
    "        print('Please select somthing in [\\'up\\', \\'down\\']')\n",
    "\n",
    "    else:\n",
    "        majority = df_func[df_func.RatingName == df_func['RatingName'].value_counts().index.tolist()[0]].reset_index(drop = True)\n",
    "        minority = df_func[df_func.RatingName == df_func['RatingName'].value_counts().index.tolist()[-1]].reset_index(drop = True)\n",
    "\n",
    "        if(sampling_type == 'up'):\n",
    "            df_func = resample(minority, replace = True, n_samples = df_func['RatingName'].value_counts().tolist()[0], random_state = 123)\n",
    "            df_func = pd.concat([majority, df_func]).sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "        if(sampling_type == 'down'):\n",
    "            df_func = resample(majority, replace=False, n_samples=df_func['RatingName'].value_counts().tolist()[-1], random_state=123) \n",
    "            df_func = pd.concat([df_func, minority]).reset_index(drop = True)\n",
    "\n",
    "        return df_func\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Reading data\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/generated.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Pre-processing\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['ID', 'Date', 'TruthRating', 'SourceURL', 'Link', 'Language'], axis = 1)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unnecessary rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting claims with OTHER é MIXTURE RatingName\n",
    "df = df[df.RatingName != 'OTHER']\n",
    "df = df[df.RatingName != 'MIXTURE']\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing \"Unknown\" & NaN by \"Inconnue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    df[column].replace(to_replace = 'Unknown', value = 'Inconnue', inplace = True)\n",
    "    df[column].replace(np.NaN, 'Inconnue', inplace = True)\n",
    "    \n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClean = df.copy()\n",
    "for column in dfClean.columns:\n",
    "    dfClean[column] = dfClean[column].apply(lambda x: clean_text(x))\n",
    "\n",
    "display(dfClean.head())\n",
    "display(dfClean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanUpsample = sampling(dfClean, sample = 'up')\n",
    "display(dfCleanUpsample.head())\n",
    "display(dfCleanUpsample.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanDownsample = sampling(dfClean, sample = 'down')\n",
    "display(dfCleanDownsample.head())\n",
    "display(dfCleanDownsample.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have df, dfClean, dfCleanUpsample and dfCleaneDownsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Encodage\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classLabelEncoder = LabelEncoder()\n",
    "\n",
    "tfidfVectorizer1 = TfidfVectorizer()\n",
    "tfidfVectorizer2 = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "tfidfTransformer1 = TfidfTransformer()\n",
    "tfidfTransformer2 = TfidfTransformer(use_idf = False)\n",
    "\n",
    "countVectorizer = CountVectorizer()\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "minMaxScaler = MinMaxScaler()\n",
    "\n",
    "# How to use\n",
    "#df = pd.DataFrame(countVectorizer.fit_transform(df), columns = ['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df\n",
    "dfHeadlineText = df[\"Headline\"] + \" \" + df[\"Text\"]\n",
    "dfRatingName = df['RatingName']\n",
    "dfAuthor = df['Author']\n",
    "dfNamedEntitiesClaim = df['NamedEntitiesClaim']\n",
    "dfNamedEntitiesArticle = df['NamedEntitiesArticle']\n",
    "dfKeywords = df['Keywords']\n",
    "dfSource = df['Source']\n",
    "\n",
    "# dfClean\n",
    "dfCleanHeadlineText = dfClean[\"Headline\"] + \" \" + dfClean[\"Text\"]\n",
    "dfCleanRatingName = dfClean['RatingName']\n",
    "dfCleanAuthor = dfClean['Author']\n",
    "dfCleanNamedEntitiesClaim = dfClean['NamedEntitiesClaim']\n",
    "dfCleanNamedEntitiesArticle = dfClean['NamedEntitiesArticle']\n",
    "dfCleanKeywords = dfClean['Keywords']\n",
    "dfCleanSource = dfClean['Source']\n",
    "\n",
    "# dfCleanUpsample\n",
    "dfCleanUpsampleHeadlineText = dfCleanUpsample[\"Headline\"] + \" \" + dfCleanUpsample[\"Text\"]\n",
    "dfCleanUpsampleRatingName = dfCleanUpsample['RatingName']\n",
    "dfCleanUpsampleAuthor = dfCleanUpsample['Author']\n",
    "dfCleanUpsampleNamedEntitiesClaim = dfCleanUpsample['NamedEntitiesClaim']\n",
    "dfCleanUpsampleNamedEntitiesArticle = dfCleanUpsample['NamedEntitiesArticle']\n",
    "dfCleanUpsampleKeywords = dfCleanUpsample['Keywords']\n",
    "dfCleanUpsampleSource = dfCleanUpsample['Source']\n",
    "\n",
    "# dfCleanDownsample\n",
    "dfCleanDownsampleHeadlineText = dfCleanDownsample[\"Headline\"] + \" \" + dfCleanDownsample[\"Text\"]\n",
    "dfCleanDownsampleRatingName = dfCleanDownsample['RatingName']\n",
    "dfCleanDownsampleAuthor = dfCleanDownsample['Author']\n",
    "dfCleanDownsampleNamedEntitiesClaim = dfCleanDownsample['NamedEntitiesClaim']\n",
    "dfCleanDownsampleNamedEntitiesArticle = dfCleanDownsample['NamedEntitiesArticle']\n",
    "dfCleanDownsampleKeywords = dfCleanDownsample['Keywords']\n",
    "dfCleanDownsampleSource = dfCleanDownsample['Source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        TF 1\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHeadlineText1 = dfHeadlineText.copy()\n",
    "dfAuthor1 = dfAuthor.copy()\n",
    "dfNamedEntitiesClaim1 = dfNamedEntitiesClaim.copy()\n",
    "dfNamedEntitiesArticle1 = dfNamedEntitiesArticle.copy()\n",
    "dfKeywords1 = dfKeywords.copy()\n",
    "dfSource1 = dfSource.copy()\n",
    "dfRatingName1 = dfRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHeadlineText1 = pd.DataFrame(classLabelEncoder.fit_transform(dfHeadlineText1), columns = ['HeadlineText'])\n",
    "dfHeadlineText1 = pd.DataFrame(standardScaler.fit_transform(dfHeadlineText1), columns = ['HeadlineText'])\n",
    "\n",
    "dfAuthor1 = pd.DataFrame(classLabelEncoder.fit_transform(dfAuthor1), columns = ['Author'])\n",
    "dfAuthor1 = pd.DataFrame(standardScaler.fit_transform(dfAuthor1), columns = ['Author'])\n",
    "\n",
    "dfNamedEntitiesClaim1 = pd.DataFrame(classLabelEncoder.fit_transform(dfNamedEntitiesClaim1), columns = ['NamedEntitiesClaim'])\n",
    "dfNamedEntitiesClaim1 = pd.DataFrame(standardScaler.fit_transform(dfNamedEntitiesClaim1), columns = ['NamedEntitiesClaim'])\n",
    "\n",
    "dfNamedEntitiesArticle1 = pd.DataFrame(classLabelEncoder.fit_transform(dfNamedEntitiesArticle1), columns = ['NamedEntitiesArticle'])\n",
    "dfNamedEntitiesArticle1 = pd.DataFrame(standardScaler.fit_transform(dfNamedEntitiesArticle1), columns = ['NamedEntitiesArticle'])\n",
    "\n",
    "dfKeywords1 = pd.DataFrame(classLabelEncoder.fit_transform(dfKeywords1), columns = ['Keywords'])\n",
    "dfKeywords1 = pd.DataFrame(standardScaler.fit_transform(dfKeywords1), columns = ['Keywords'])\n",
    "\n",
    "dfSource1 = pd.DataFrame(classLabelEncoder.fit_transform(dfSource1), columns = ['Source'])\n",
    "dfSource1 = pd.DataFrame(standardScaler.fit_transform(dfSource1), columns = ['Source'])\n",
    "\n",
    "dfRatingName1 = pd.DataFrame(classLabelEncoder.fit_transform(dfRatingName1), columns = ['RatingName'])\n",
    "\n",
    "df1 = pd.concat([dfHeadlineText1, dfAuthor1, dfNamedEntitiesClaim1, dfNamedEntitiesArticle1, dfKeywords1, dfSource1, dfRatingName1], axis = 1)\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('attemps/tf1.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        TF cleaned 1\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanHeadlineText1 = dfCleanHeadlineText.copy()\n",
    "dfCleanAuthor1 = dfCleanAuthor.copy()\n",
    "dfCleanNamedEntitiesClaim1 = dfCleanNamedEntitiesClaim.copy()\n",
    "dfCleanNamedEntitiesArticle1 = dfCleanNamedEntitiesArticle.copy()\n",
    "dfCleanKeywords1 = dfCleanKeywords.copy()\n",
    "dfCleanSource1 = dfCleanSource.copy()\n",
    "dfCleanRatingName1 = dfCleanRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanHeadlineText1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanHeadlineText1), columns = ['HeadlineText'])\n",
    "dfCleanHeadlineText1 = pd.DataFrame(standardScaler.fit_transform(dfCleanHeadlineText1), columns = ['HeadlineText'])\n",
    "\n",
    "dfCleanAuthor1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanAuthor1), columns = ['Author'])\n",
    "dfCleanAuthor1 = pd.DataFrame(standardScaler.fit_transform(dfCleanAuthor1), columns = ['Author'])\n",
    "\n",
    "dfCleanNamedEntitiesClaim1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanNamedEntitiesClaim1), columns = ['NamedEntitiesClaim'])\n",
    "dfCleanNamedEntitiesClaim1 = pd.DataFrame(standardScaler.fit_transform(dfCleanNamedEntitiesClaim1), columns = ['NamedEntitiesClaim'])\n",
    "\n",
    "dfCleanNamedEntitiesArticle1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanNamedEntitiesArticle1), columns = ['NamedEntitiesArticle'])\n",
    "dfCleanNamedEntitiesArticle1 = pd.DataFrame(standardScaler.fit_transform(dfCleanNamedEntitiesArticle1), columns = ['NamedEntitiesArticle'])\n",
    "\n",
    "dfCleanKeywords1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanKeywords1), columns = ['Keywords'])\n",
    "dfCleanKeywords1 = pd.DataFrame(standardScaler.fit_transform(dfCleanKeywords1), columns = ['Keywords'])\n",
    "\n",
    "dfCleanSource1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanSource1), columns = ['Source'])\n",
    "dfCleanSource1 = pd.DataFrame(standardScaler.fit_transform(dfCleanSource1), columns = ['Source'])\n",
    "\n",
    "dfCleanRatingName1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanRatingName1), columns = ['RatingName'])\n",
    "\n",
    "dfClean1 = pd.concat([dfCleanHeadlineText1, dfCleanAuthor1, dfCleanNamedEntitiesClaim1, dfCleanNamedEntitiesArticle1, dfCleanKeywords1, dfCleanSource1, dfCleanRatingName1], axis = 1)\n",
    "\n",
    "display(dfClean1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClean1.to_csv('attemps/tfclean1.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        TF cleaned upsampled 1\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanUpsampleHeadlineText1 = dfCleanUpsampleHeadlineText.copy()\n",
    "dfCleanUpsampleAuthor1 = dfCleanUpsampleAuthor.copy()\n",
    "dfCleanUpsampleNamedEntitiesClaim1 = dfCleanUpsampleNamedEntitiesClaim.copy()\n",
    "dfCleanUpsampleNamedEntitiesArticle1 = dfCleanUpsampleNamedEntitiesArticle.copy()\n",
    "dfCleanUpsampleKeywords1 = dfCleanUpsampleKeywords.copy()\n",
    "dfCleanUpsampleSource1 = dfCleanUpsampleSource.copy()\n",
    "dfCleanUpsampleRatingName1 = dfCleanUpsampleRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanUpsampleHeadlineText1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanUpsampleHeadlineText1), columns = ['HeadlineText'])\n",
    "dfCleanUpsampleHeadlineText1 = pd.DataFrame(standardScaler.fit_transform(dfCleanUpsampleHeadlineText1), columns = ['HeadlineText'])\n",
    "\n",
    "dfCleanUpsampleAuthor1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanUpsampleAuthor1), columns = ['Author'])\n",
    "dfCleanUpsampleAuthor1 = pd.DataFrame(standardScaler.fit_transform(dfCleanUpsampleAuthor1), columns = ['Author'])\n",
    "\n",
    "dfCleanUpsampleNamedEntitiesClaim1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanUpsampleNamedEntitiesClaim1), columns = ['NamedEntitiesClaim'])\n",
    "dfCleanUpsampleNamedEntitiesClaim1 = pd.DataFrame(standardScaler.fit_transform(dfCleanUpsampleNamedEntitiesClaim1), columns = ['NamedEntitiesClaim'])\n",
    "\n",
    "dfCleanUpsampleNamedEntitiesArticle1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanUpsampleNamedEntitiesArticle1), columns = ['NamedEntitiesArticle'])\n",
    "dfCleanUpsampleNamedEntitiesArticle1 = pd.DataFrame(standardScaler.fit_transform(dfCleanUpsampleNamedEntitiesArticle1), columns = ['NamedEntitiesArticle'])\n",
    "\n",
    "dfCleanUpsampleKeywords1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanUpsampleKeywords1), columns = ['Keywords'])\n",
    "dfCleanUpsampleKeywords1 = pd.DataFrame(standardScaler.fit_transform(dfCleanUpsampleKeywords1), columns = ['Keywords'])\n",
    "\n",
    "dfCleanUpsampleSource1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanUpsampleSource1), columns = ['Source'])\n",
    "dfCleanUpsampleSource1 = pd.DataFrame(standardScaler.fit_transform(dfCleanUpsampleSource1), columns = ['Source'])\n",
    "\n",
    "dfCleanUpsampleRatingName1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanUpsampleRatingName1), columns = ['RatingName'])\n",
    "\n",
    "dfCleanUpsample1 = pd.concat([dfCleanUpsampleHeadlineText1, dfCleanUpsampleAuthor1, dfCleanUpsampleNamedEntitiesClaim1, dfCleanUpsampleNamedEntitiesArticle1, dfCleanUpsampleKeywords1, dfCleanUpsampleSource1, dfCleanUpsampleRatingName1], axis = 1)\n",
    "\n",
    "display(dfCleanUpsample1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanUpsample1.to_csv('attemps/tfcleanupsample1.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        TF cleaned downsampled 1\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanDownsampleHeadlineText1 = dfCleanDownsampleHeadlineText.copy()\n",
    "dfCleanDownsampleAuthor1 = dfCleanDownsampleAuthor.copy()\n",
    "dfCleanDownsampleNamedEntitiesClaim1 = dfCleanDownsampleNamedEntitiesClaim.copy()\n",
    "dfCleanDownsampleNamedEntitiesArticle1 = dfCleanDownsampleNamedEntitiesArticle.copy()\n",
    "dfCleanDownsampleKeywords1 = dfCleanDownsampleKeywords.copy()\n",
    "dfCleanDownsampleSource1 = dfCleanDownsampleSource.copy()\n",
    "dfCleanDownsampleRatingName1 = dfCleanDownsampleRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanDownsampleHeadlineText1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanDownsampleHeadlineText1), columns = ['HeadlineText'])\n",
    "dfCleanDownsampleHeadlineText1 = pd.DataFrame(standardScaler.fit_transform(dfCleanDownsampleHeadlineText1), columns = ['HeadlineText'])\n",
    "\n",
    "dfCleanDownsampleAuthor1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanDownsampleAuthor1), columns = ['Author'])\n",
    "dfCleanDownsampleAuthor1 = pd.DataFrame(standardScaler.fit_transform(dfCleanDownsampleAuthor1), columns = ['Author'])\n",
    "\n",
    "dfCleanDownsampleNamedEntitiesClaim1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanDownsampleNamedEntitiesClaim1), columns = ['NamedEntitiesClaim'])\n",
    "dfCleanDownsampleNamedEntitiesClaim1 = pd.DataFrame(standardScaler.fit_transform(dfCleanDownsampleNamedEntitiesClaim1), columns = ['NamedEntitiesClaim'])\n",
    "\n",
    "dfCleanDownsampleNamedEntitiesArticle1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanDownsampleNamedEntitiesArticle1), columns = ['NamedEntitiesArticle'])\n",
    "dfCleanDownsampleNamedEntitiesArticle1 = pd.DataFrame(standardScaler.fit_transform(dfCleanDownsampleNamedEntitiesArticle1), columns = ['NamedEntitiesArticle'])\n",
    "\n",
    "dfCleanDownsampleKeywords1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanDownsampleKeywords1), columns = ['Keywords'])\n",
    "dfCleanDownsampleKeywords1 = pd.DataFrame(standardScaler.fit_transform(dfCleanDownsampleKeywords1), columns = ['Keywords'])\n",
    "\n",
    "dfCleanDownsampleSource1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanDownsampleSource1), columns = ['Source'])\n",
    "dfCleanDownsampleSource1 = pd.DataFrame(standardScaler.fit_transform(dfCleanDownsampleSource1), columns = ['Source'])\n",
    "\n",
    "dfCleanDownsampleRatingName1 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanDownsampleRatingName1), columns = ['RatingName'])\n",
    "\n",
    "dfCleanDownsample1 = pd.concat([dfCleanDownsampleHeadlineText1, dfCleanDownsampleAuthor1, dfCleanDownsampleNamedEntitiesClaim1, dfCleanDownsampleNamedEntitiesArticle1, dfCleanDownsampleKeywords1, dfCleanDownsampleSource1, dfCleanDownsampleRatingName1], axis = 1)\n",
    "\n",
    "display(dfCleanDownsample1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanDownsample1.to_csv('attemps/tfcleandownsample1.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        TF2\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHeadlineText2 = dfHeadlineText.copy()\n",
    "dfAuthor2 = dfAuthor.copy()\n",
    "dfNamedEntitiesClaim2 = dfNamedEntitiesClaim.copy()\n",
    "dfNamedEntitiesArticle2 = dfNamedEntitiesArticle.copy()\n",
    "dfKeywords2 = dfKeywords.copy()\n",
    "dfSource2 = dfSource.copy()\n",
    "dfRatingName2 = dfRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfHeadlineText2 = pd.DataFrame(tfidfVectorizer1.fit_transform(dfHeadlineText2).toarray(), columns = tfidfVectorizer1.get_feature_names())\n",
    "dfHeadlineText2 = pd.DataFrame(standardScaler.fit_transform(dfHeadlineText2), columns = tfidfVectorizer1.get_feature_names())\n",
    "\n",
    "dfSource2 = pd.get_dummies(dfSource2, columns = ['Source'], prefix = 'Source').reset_index(drop = True)\n",
    "\n",
    "dfRatingName2 = pd.DataFrame(classLabelEncoder.fit_transform(dfRatingName2), columns = ['RatingName'])\n",
    "\n",
    "df2 = pd.concat([dfHeadlineText2, dfSource2, dfRatingName2], axis = 1)\n",
    "\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('attemps/tf2.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        TF cleaned 2\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanHeadlineText2 = dfCleanHeadlineText.copy()\n",
    "dfCleanAuthor2 = dfCleanAuthor.copy()\n",
    "dfCleanNamedEntitiesClaim2 = dfCleanNamedEntitiesClaim.copy()\n",
    "dfCleanNamedEntitiesArticle2 = dfCleanNamedEntitiesArticle.copy()\n",
    "dfCleanKeywords2 = dfCleanKeywords.copy()\n",
    "dfCleanSource2 = dfCleanSource.copy()\n",
    "dfCleanRatingName2 = dfCleanRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfCleanHeadlineText2 = pd.DataFrame(tfidfVectorizer1.fit_transform(dfCleanHeadlineText2).toarray(), columns = tfidfVectorizer1.get_feature_names())\n",
    "dfCleanHeadlineText2 = pd.DataFrame(standardScaler.fit_transform(dfCleanHeadlineText2), columns = tfidfVectorizer1.get_feature_names())\n",
    "\n",
    "dfCleanSource2 = pd.get_dummies(dfCleanSource2, columns = ['Source'], prefix = 'Source').reset_index(drop = True)\n",
    "\n",
    "dfCleanRatingName2 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanRatingName2), columns = ['RatingName'])\n",
    "\n",
    "dfClean2 = pd.concat([dfCleanHeadlineText2, dfCleanSource2, dfCleanRatingName2], axis = 1)\n",
    "\n",
    "display(dfClean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClean2.to_csv('attemps/tfclean2.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        TF cleaned upsampled 2\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanUpsampleHeadlineText2 = dfCleanUpsampleHeadlineText.copy()\n",
    "dfCleanUpsampleAuthor2 = dfCleanUpsampleAuthor.copy()\n",
    "dfCleanUpsampleNamedEntitiesClaim2 = dfCleanUpsampleNamedEntitiesClaim.copy()\n",
    "dfCleanUpsampleNamedEntitiesArticle2 = dfCleanUpsampleNamedEntitiesArticle.copy()\n",
    "dfCleanUpsampleKeywords2 = dfCleanUpsampleKeywords.copy()\n",
    "dfCleanUpsampleSource2 = dfCleanUpsampleSource.copy()\n",
    "dfCleanUpsampleRatingName2 = dfCleanUpsampleRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfCleanUpsampleHeadlineText2 = pd.DataFrame(tfidfVectorizer1.fit_transform(dfCleanUpsampleHeadlineText2).toarray(), columns = tfidfVectorizer1.get_feature_names())\n",
    "dfCleanUpsampleHeadlineText2 = pd.DataFrame(standardScaler.fit_transform(dfCleanUpsampleHeadlineText2), columns = tfidfVectorizer1.get_feature_names())\n",
    "\n",
    "dfCleanUpsampleSource2 = pd.get_dummies(dfCleanUpsampleSource2, columns = ['Source'], prefix = 'Source').reset_index(drop = True)\n",
    "\n",
    "dfCleanUpsampleRatingName2 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanUpsampleRatingName2), columns = ['RatingName'])\n",
    "\n",
    "dfCleanUpsample2 = pd.concat([dfCleanUpsampleHeadlineText2, dfCleanUpsampleSource2, dfCleanUpsampleRatingName2], axis = 1)\n",
    "\n",
    "display(dfCleanUpsample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanUpsample2.to_csv('attemps/tfcleanupsample2.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        TF cleaned downsampled 2\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanDownsampleHeadlineText2 = dfCleanDownsampleHeadlineText.copy()\n",
    "dfCleanDownsampleAuthor2 = dfCleanDownsampleAuthor.copy()\n",
    "dfCleanDownsampleNamedEntitiesClaim2 = dfCleanDownsampleNamedEntitiesClaim.copy()\n",
    "dfCleanDownsampleNamedEntitiesArticle2 = dfCleanDownsampleNamedEntitiesArticle.copy()\n",
    "dfCleanDownsampleKeywords2 = dfCleanDownsampleKeywords.copy()\n",
    "dfCleanDownsampleSource2 = dfCleanDownsampleSource.copy()\n",
    "dfCleanDownsampleRatingName2 = dfCleanDownsampleRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfCleanDownsampleHeadlineText2 = pd.DataFrame(tfidfVectorizer1.fit_transform(dfCleanDownsampleHeadlineText2).toarray(), columns = tfidfVectorizer1.get_feature_names())\n",
    "dfCleanDownsampleHeadlineText2 = pd.DataFrame(standardScaler.fit_transform(dfCleanDownsampleHeadlineText2), columns = tfidfVectorizer1.get_feature_names())\n",
    "\n",
    "dfCleanDownsampleSource2 = pd.get_dummies(dfCleanDownsampleSource2, columns = ['Source'], prefix = 'Source').reset_index(drop = True)\n",
    "\n",
    "dfCleanDownsampleRatingName2 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanDownsampleRatingName2), columns = ['RatingName'])\n",
    "\n",
    "dfCleanDownsample2 = pd.concat([dfCleanDownsampleHeadlineText2, dfCleanDownsampleSource2, dfCleanDownsampleRatingName2], axis = 1)\n",
    "\n",
    "display(dfCleanDownsample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanDownsample2.to_csv('attemps/tfcleandownsample2.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        TF3\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHeadlineText3 = dfHeadlineText.copy()\n",
    "dfAuthor3 = dfAuthor.copy()\n",
    "dfNamedEntitiesClaim3 = dfNamedEntitiesClaim.copy()\n",
    "dfNamedEntitiesArticle3 = dfNamedEntitiesArticle.copy()\n",
    "dfKeywords3 = dfKeywords.copy()\n",
    "dfSource3 = dfSource.copy()\n",
    "dfRatingName3 = dfRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfHeadlineText3 = pd.DataFrame(tfidfVectorizer1.fit_transform(dfHeadlineText3).toarray(), columns = tfidfVectorizer1.get_feature_names())\n",
    "#dfHeadlineText3 = pd.DataFrame(tfidfTransformer1.fit_transform(dfHeadlineText3).toarray(), columns = tfidfVectorizer1.get_feature_names())\n",
    "dfHeadlineText3 = pd.DataFrame(standardScaler.fit_transform(dfHeadlineText3), columns = tfidfVectorizer1.get_feature_names())\n",
    "\n",
    "dfSource3 = pd.get_dummies(dfSource3, columns = ['Source'], prefix = 'Source').reset_index(drop = True)\n",
    "\n",
    "dfKeywords3 = pd.DataFrame(data = countVectorizer.fit_transform(dfKeywords3).toarray(), columns = countVectorizer.get_feature_names())\n",
    "\n",
    "dfRatingName3 = pd.DataFrame(classLabelEncoder.fit_transform(dfRatingName3), columns = ['RatingName'])\n",
    "\n",
    "# Pas encore traité\n",
    "#dfCleanDownsampleAuthor2 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanDownsampleAuthor2), columns = ['Author'])\n",
    "#dfCleanDownsampleAuthor2 = pd.DataFrame(minMaxScaler.fit_transform(dfCleanDownsampleAuthor2), columns = ['Author'])\n",
    "\n",
    "#dfCleanDownsampleNamedEntitiesClaim2 = pd.DataFrame(tfidfVectorizer.fit_transform(dfCleanDownsampleNamedEntitiesClaim2).toarray(), columns = tfidfVectorizer.get_feature_names())\n",
    "#dfCleanDownsampleNamedEntitiesClaim2 = pd.DataFrame(standardScaler.fit_transform(dfCleanDownsampleNamedEntitiesClaim2), columns = tfidfVectorizer.get_feature_names())\n",
    "\n",
    "#dfCleanDownsampleNamedEntitiesArticle2 = pd.DataFrame(classLabelEncoder.fit_transform(dfCleanDownsampleNamedEntitiesArticle2), columns = ['NamedEntitiesArticle'])\n",
    "#dfCleanDownsampleNamedEntitiesArticle2 = pd.DataFrame(minMaxScaler.fit_transform(dfCleanDownsampleNamedEntitiesArticle2), columns = ['NamedEntitiesArticle'])\n",
    "\n",
    "df3 = pd.concat([dfHeadlineText3, dfKeywords3, dfSource3, dfRatingName3], axis = 1)\n",
    "\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('attemps/tf3.csv', sep = ';', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
