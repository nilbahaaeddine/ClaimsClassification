{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Projet Data Science </h1></div>\n",
    "<div align=\"center\"><h2> Classification d'assertions selon leur valeurs de véracité ( automatic fact-checking ) </h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Imports\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "from enum import Enum\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Loading the dataset\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = [\"ID\", \"ClaimReviewAuthor\", \"ClaimReviewAuthorName\", \"ClaimReviewAuthorURL\",\n",
    "          \"ClaimReviewClaimReviewed\", \"ClaimReviewDatePublished\", \"ClaimReviewSource\", \"ClaimReviewURL\",\n",
    "          \"CreativeWorkAuthorName\", \"CreativeWorkAuthorSameAs\", \"CreativeWorkDatePublished\", \"ExtraBody\",\n",
    "          \"ExtraEntitiesAuthor\", \"ExtraEntitiesBody\", \"ExtraEntitiesClaimReviewClaimReviewed\", \"ExtraEntitiesKeywords\",\n",
    "          \"ExtraReferedLinks\", \"ExtraTags\", \"ExtraTitle\", \"RatingAlternateName\",\n",
    "          \"RatingBestRating\", \"RatingRatingValue\", \"RatingWorstRating\"]\n",
    "\n",
    "labelsClaimsKG = [\"ID\", \"Text\", \"Date\", \"TruthRating\", \"RatingName\", \"Author\", \"Headline\",\n",
    "                  \"NamedEntitiesClaim\", \"NamedEntitiesArticle\", \"Keywords\", \"Source\", \"SourceURL\", \"Link\", \"Language\"]\n",
    "\n",
    "df = pd.read_csv('datasets/ClaimsKG.csv', sep = ',', names = labelsClaimsKG, skiprows = 1, nrows = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Printing informations\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape :\\n{df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Informations :')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Description :')\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing some lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Printing some lines :')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage d'informations sur toutes les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "      print(f'Nombre de valeurs nulles pour {column} :\\n{df[column].isnull().value_counts()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage des colonnes vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = []\n",
    "for column in df.columns:\n",
    "    value = False\n",
    "    value = df[column].isnull().any()\n",
    "    if value:\n",
    "        array.append(column)\n",
    "print(f'Nombre de colonnes vides : {len(array)}\\nLes colonnes vide sont :\\n{array}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description de toutes les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    display(df[column].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a series of unique values in each column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    uniqueValues = df[column].unique()\n",
    "    print(f'Number of unique elements in column {column} : {len(uniqueValues)}, values & type :\\n{uniqueValues}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage du nombre des différents TruthRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'La colonne TruthRating contient :')\n",
    "print(f'{df[df[\"TruthRating\"]==-1][\"ID\"].count()} Other')\n",
    "print(f'{df[df[\"TruthRating\"]==1][\"ID\"].count()} False')\n",
    "print(f'{df[df[\"TruthRating\"]==2][\"ID\"].count()} Mixture')\n",
    "print(f'{df[df[\"TruthRating\"]==3][\"ID\"].count()} True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Visualization\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.countplot(x = 'Source', data = df)\n",
    "plt.setp(chart.get_xticklabels(), rotation = 45, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot(x = 'Source', col = 'RatingName', kind = 'count', data = df)\n",
    "for ax in chart.axes.ravel():\n",
    "    plt.setp(ax.get_xticklabels(), rotation = 45, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot('Source', data = df, hue = 'RatingName', kind = 'count')\n",
    "for ax in chart.axes.ravel():\n",
    "    plt.setp(ax.get_xticklabels(), rotation = 45, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), cbar = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Mapping ratings\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedRatings(Enum):\n",
    "    FALSE = 1\n",
    "    MIXTURE = 2\n",
    "    TRUE = 3\n",
    "    OTHER = -1\n",
    "\n",
    "_normalization_dictionary = {  # type: Dict[str, Dict[str,NormalizedRatings]]\n",
    "    \"politifact\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'incorrect': NormalizedRatings.FALSE,\n",
    "        'pants-fire': NormalizedRatings.FALSE,\n",
    "        'pants on fire': NormalizedRatings.FALSE,\n",
    "        'pants on fire!': NormalizedRatings.FALSE,\n",
    "        'false': NormalizedRatings.FALSE,\n",
    "        'mostly correct': NormalizedRatings.MIXTURE,\n",
    "        'mostly false': NormalizedRatings.MIXTURE,\n",
    "        'barely true': NormalizedRatings.MIXTURE,\n",
    "        'half true': NormalizedRatings.MIXTURE,\n",
    "        'half-true': NormalizedRatings.MIXTURE,\n",
    "        'mostly true': NormalizedRatings.MIXTURE,\n",
    "        'true': NormalizedRatings.TRUE,\n",
    "        'correct': NormalizedRatings.TRUE\n",
    "    },\n",
    "    \"snopes\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'false': NormalizedRatings.FALSE,\n",
    "        'legend': NormalizedRatings.FALSE,\n",
    "        'mixture': NormalizedRatings.MIXTURE,\n",
    "        'mixture:': NormalizedRatings.MIXTURE,\n",
    "        'true': NormalizedRatings.TRUE,\n",
    "        'mostly false': NormalizedRatings.MIXTURE,\n",
    "        'mostly true': NormalizedRatings.MIXTURE,\n",
    "        'partly true': NormalizedRatings.MIXTURE,\n",
    "        'MIXTURE OF TRUE AND FALSE INFORMATION': NormalizedRatings.MIXTURE,\n",
    "        'MIXTURE OF TRUE AND FALSE INFORMATION:': NormalizedRatings.MIXTURE,\n",
    "        'MIXTURE OF ACCURATE AND  INACCURATE INFORMATION': NormalizedRatings.MIXTURE\n",
    "    },\n",
    "    \"africacheck\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'incorrect': NormalizedRatings.FALSE,\n",
    "        'mostly-correct': NormalizedRatings.MIXTURE,\n",
    "        'correct': NormalizedRatings.TRUE\n",
    "    },\n",
    "    \"factscan\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'false': NormalizedRatings.FALSE,\n",
    "        'true': NormalizedRatings.TRUE,\n",
    "        'Misleading': NormalizedRatings.OTHER\n",
    "    },\n",
    "    \"truthorfiction\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'fiction': NormalizedRatings.FALSE,\n",
    "        'truth': NormalizedRatings.TRUE,\n",
    "        'truth & fiction': NormalizedRatings.MIXTURE,\n",
    "        'mostly fiction': NormalizedRatings.MIXTURE,\n",
    "        'truth & misleading': NormalizedRatings.MIXTURE,\n",
    "        'mostly truth': NormalizedRatings.MIXTURE\n",
    "    },\n",
    "    \"checkyourfact\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'False': NormalizedRatings.FALSE,\n",
    "        'True': NormalizedRatings.TRUE,\n",
    "        'Mostly True': NormalizedRatings.MIXTURE,\n",
    "        'true/false': NormalizedRatings.MIXTURE,\n",
    "        'truth & misleading': NormalizedRatings.MIXTURE,\n",
    "        'mostly truth': NormalizedRatings.MIXTURE,\n",
    "        'misleading': NormalizedRatings.MIXTURE\n",
    "    },\n",
    "    \"factcheck_aap\": {\n",
    "        \"True\": NormalizedRatings.TRUE,\n",
    "        \"False\": NormalizedRatings.FALSE,\n",
    "        \"Mostly True\": NormalizedRatings.MIXTURE,\n",
    "        \"Mostly False\": NormalizedRatings.MIXTURE,\n",
    "        \"Somewhat True\": NormalizedRatings.MIXTURE,\n",
    "        \"Somewhat False\": NormalizedRatings.MIXTURE\n",
    "    },\n",
    "    \"factuel_afp_fr\": {\n",
    "        'Faux': NormalizedRatings.FALSE,\n",
    "        'Totalement faux': NormalizedRatings.FALSE,\n",
    "        'Démenti': NormalizedRatings.FALSE,\n",
    "        \"C'est une oeuvre de fiction\": NormalizedRatings.FALSE,\n",
    "        'Vrai': NormalizedRatings.TRUE,\n",
    "        'Totalement Vrai': NormalizedRatings.TRUE,\n",
    "        'Plutôt vrai': NormalizedRatings.MIXTURE,\n",
    "        'Trompeur': NormalizedRatings.MIXTURE,\n",
    "        'trompeur': NormalizedRatings.MIXTURE,\n",
    "        'Plutôt faux': NormalizedRatings.MIXTURE,\n",
    "        'Presque': NormalizedRatings.MIXTURE,\n",
    "        'Mélangé': NormalizedRatings.MIXTURE,\n",
    "        'Mélange': NormalizedRatings.MIXTURE,\n",
    "        'Inexact': NormalizedRatings.MIXTURE,\n",
    "        'Incertain': NormalizedRatings.MIXTURE,\n",
    "        'Imprécis': NormalizedRatings.MIXTURE,\n",
    "        'Exagéré': NormalizedRatings.MIXTURE,\n",
    "        'Douteux': NormalizedRatings.MIXTURE,\n",
    "    },\n",
    "    \"factcheck_afp\": {\n",
    "        'False': NormalizedRatings.FALSE,\n",
    "        'Fake': NormalizedRatings.FALSE,\n",
    "        'Mixed': NormalizedRatings.MIXTURE,\n",
    "        'Hoax': NormalizedRatings.FALSE,\n",
    "        'Falso': NormalizedRatings.FALSE,\n",
    "        'APRIL FOOL': NormalizedRatings.FALSE\n",
    "    },\n",
    "    \"fullfact\": {\n",
    "        'Correct': NormalizedRatings.TRUE,\n",
    "        'Incorrect': NormalizedRatings.FALSE,\n",
    "        'Not quite': NormalizedRatings.MIXTURE\n",
    "    }\n",
    "}\n",
    "\n",
    "def _standardize_name(original_name: str):\n",
    "    return original_name.strip().lower().replace(\"!\", \"\").replace(\":\", \"\").replace(\"-\", \" \")\n",
    "\n",
    "def normalize(source_name, original_name) -> NormalizedRatings:\n",
    "    '''\n",
    "    Generate a normalized rating from the original ratings on each respective site\n",
    "    :param original_name:\n",
    "    :return normalized_rating: NormalizedRating\n",
    "    '''\n",
    "    try:\n",
    "        source = _normalization_dictionary[source_name]\n",
    "        normalized_value = source[_standardize_name(original_name)]\n",
    "    except KeyError:\n",
    "        normalized_value = NormalizedRatings.OTHER\n",
    "    return normalized_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Pre-processing\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        General pre-processing\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['TruthRating', 'SourceURL', 'Language'], axis = 1)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting claims with OTHER RatingName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.RatingName != 'OTHER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing \"Unknown\" & NaN by \"Inconnue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    df[column].replace(to_replace = 'Unknown', value = 'Inconnue', inplace = True)\n",
    "    df[column].replace(np.NaN, 'Inconnue', inplace = True)\n",
    "    \n",
    "display(df.head())\n",
    "\n",
    "sns.heatmap(df.isnull(), cbar = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRUE VS FALSE datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTvsF = df.copy()\n",
    "\n",
    "# Suppression de MIXTURE\n",
    "dfTvsF = dfTvsF[dfTvsF.RatingName != 'MIXTURE']\n",
    "\n",
    "replace_map = {'FALSE': 1, 'TRUE': 2}\n",
    "\n",
    "# Creating a new column with new RatingName (Prediction)\n",
    "dfTvsF[\"Predection\"] = dfTvsF['RatingName'].map(replace_map)\n",
    "\n",
    "# Removing RatingName\n",
    "dfTvsF = dfTvsF.drop(['RatingName'], axis = 1)\n",
    "\n",
    "display(dfTvsF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRUE/FALSE VS MIXTURE datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTFvsM = df.copy()\n",
    "\n",
    "replace_map = {'FALSE': 1, 'TRUE': 1, 'MIXTURE': 2}\n",
    "\n",
    "# Creating a new column with new RatingName (Prediction)\n",
    "dfTFvsM[\"Predection\"] = dfTFvsM['RatingName'].map(replace_map)\n",
    "\n",
    "# Removing RatingName\n",
    "dfTFvsM = dfTFvsM.drop(['RatingName'], axis = 1)\n",
    "\n",
    "display(dfTFvsM.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        First attemp on TRUE vs FALSE\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTvsF1 = dfTvsF.copy()\n",
    "display(dfTvsF1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classLabelEncoder = LabelEncoder()\n",
    "\n",
    "dfTvsF1[\"ID\"]=classLabelEncoder.fit_transform(dfTvsF1[\"ID\"])\n",
    "dfTvsF1[\"Text\"]=classLabelEncoder.fit_transform(dfTvsF1[\"Text\"])\n",
    "dfTvsF1[\"Date\"]=classLabelEncoder.fit_transform(dfTvsF1[\"Date\"])\n",
    "dfTvsF1[\"Author\"]=classLabelEncoder.fit_transform(dfTvsF1[\"Author\"])\n",
    "dfTvsF1[\"Headline\"]=classLabelEncoder.fit_transform(dfTvsF1[\"Headline\"])\n",
    "dfTvsF1[\"NamedEntitiesClaim\"]=classLabelEncoder.fit_transform(dfTvsF1[\"NamedEntitiesClaim\"])\n",
    "dfTvsF1[\"NamedEntitiesArticle\"]=classLabelEncoder.fit_transform(dfTvsF1[\"NamedEntitiesArticle\"])\n",
    "dfTvsF1[\"Keywords\"]=classLabelEncoder.fit_transform(dfTvsF1[\"Keywords\"])\n",
    "dfTvsF1[\"Source\"]=classLabelEncoder.fit_transform(dfTvsF1[\"Source\"])\n",
    "dfTvsF1[\"Link\"]=classLabelEncoder.fit_transform(dfTvsF1[\"Link\"])\n",
    "\n",
    "display(dfTvsF1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTvsF1.to_csv('datasets/attemp1.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        Second attemp on TRUE vs FALSE\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTvsF2 = dfTvsF.copy()\n",
    "display(dfTvsF2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dfTvsF2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTvsF2.to_csv('datasets/attemp2.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Classification\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        Preparing attemp 1 data for classification\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the transformed data for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClassification1 = pd.read_csv('datasets/attemp1.csv', sep = ';')\n",
    "display(dfClassification1.head())\n",
    "dfClassification1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the learning variables and the variable to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = dfClassification1.values\n",
    "X1 = array1[:,[0,1]]\n",
    "y1 = array1[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut the data set into a test set and a learning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTrainSize = 0.3 # 30% du jeu de données pour le test\n",
    "myTestSize = 1 - myTrainSize # 70% du jeu de données pour l'entraînement\n",
    "seed = 30\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, train_size = myTrainSize, random_state = seed, test_size = myTestSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        Testing the first classifier on attemp 1\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaussianNB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfGaussianNB = GaussianNB()\n",
    "\n",
    "clfGaussianNB.fit(X_train1, y_train1)\n",
    "\n",
    "resultGaussianNB = clfGaussianNB.predict(X_test1)\n",
    "\n",
    "print(f'accuracy : {accuracy_score(resultGaussianNB, y_test1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the confusion matrix and the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Matrice de confusion :\\n{confusion_matrix(y_test1, resultGaussianNB)}')\n",
    "print (f'Classification report :\\n{classification_report(y_test1, resultGaussianNB)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validate with 10 splits (Kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "myKFold = KFold(n_splits = 10, shuffle = True, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the GaussianNB classifier and give the different accuracy for the 10 evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfGaussianNB = GaussianNB()\n",
    "\n",
    "myScoring = 'accuracy'\n",
    "\n",
    "score = cross_val_score(clfGaussianNB, X1, y1, cv = myKFold, scoring = myScoring)\n",
    "\n",
    "print(f'Les différentes accuracy pour les 10 évaluations sont :\\n{score}')\n",
    "print(f'Accuracy moyenne : {score.mean()} | Standard deviation : {score.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        Testing several classifiers\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "myScoring = 'accuracy'\n",
    "models = []\n",
    "\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma = 'auto')))\n",
    "models.append(('RFO', RandomForestClassifier()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    myKFold = KFold(n_splits = 10, random_state = seed)\n",
    "    score = cross_val_score(model, X1, y1, cv = myKFold, scoring = myScoring)\n",
    "    scores.append(score)\n",
    "    names.append(name)\n",
    "    print(f'{name} : {score.mean()} | {score.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying results of the different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Comparaison des algorithmes')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(scores)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    myKFold = KFold(n_splits = 10, shuffle = True, random_state = seed)\n",
    "    score = cross_val_score(model, X1, y1, cv = myKFold, scoring = myScoring)\n",
    "    scores.append(score)\n",
    "    names.append(name)\n",
    "    print(f'{name} : {score.mean()} | {score.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying results of the different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Comparaison des algorithmes')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(scores)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply GridSearchCV to RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridParam = {'n_estimators': [4, 6, 9], \n",
    "             'max_features': ['log2', 'sqrt','auto'], \n",
    "             'criterion': ['entropy', 'gini'], \n",
    "             'max_depth': [2, 3, 5, 10], \n",
    "             'min_samples_split': [2, 3, 5], \n",
    "             'min_samples_leaf': [1, 5, 8]\n",
    "            }\n",
    "\n",
    "myScoring = 'accuracy'\n",
    "\n",
    "clfGridSearchCV = GridSearchCV(estimator = RandomForestClassifier(), param_grid = gridParam, scoring = myScoring, cv = 5, n_jobs = -1, iid = True, return_train_score = True)\n",
    "\n",
    "clfGridSearchCV.fit(X_train1, y_train1)\n",
    "\n",
    "print(f'meilleur score : {clfGridSearchCV.best_score_}')\n",
    "print(f'meilleurs paramètres :\\n{clfGridSearchCV.best_params_}')\n",
    "print(f'meilleur estimateur :\\n{clfGridSearchCV.best_estimator_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply GridSearchCV to DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridParam = {'max_depth' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "             'criterion' : ['gini', 'entropy'], \n",
    "             'min_samples_leaf' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "            }\n",
    "\n",
    "myScoring = 'accuracy'\n",
    "\n",
    "clfGridSearchCV = GridSearchCV(estimator = DecisionTreeClassifier(), param_grid = gridParam, scoring = myScoring, cv = 10, n_jobs = -1, iid = True, return_train_score = True)\n",
    "\n",
    "clfGridSearchCV.fit(X_train1, y_train1)  \n",
    "\n",
    "print(f'meilleur score : {clfGridSearchCV.best_score_}')\n",
    "print(f'meilleurs paramètres :\\n{clfGridSearchCV.best_params_}')\n",
    "print(f'meilleur estimateur :\\n{clfGridSearchCV.best_estimator_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply GridSearchCV to SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridParam = {'C' : [0.001, 0.01, 0.1, 1, 10], \n",
    "             'gamma' : [0.001, 0.01, 0.1, 1], \n",
    "             'kernel' : ['linear', 'rbf']\n",
    "            }\n",
    "\n",
    "myScoring = 'accuracy'\n",
    "\n",
    "clfGridSearchCV = GridSearchCV(estimator = SVC(), param_grid = gridParam, scoring = myScoring, cv = 5, n_jobs = 1, iid = True, return_train_score = True)\n",
    "\n",
    "clfGridSearchCV.fit(X_train1, y_train1)\n",
    "\n",
    "print(f'meilleur score : {clfGridSearchCV.best_score_}')\n",
    "print(f'meilleurs paramètres :\\n{clfGridSearchCV.best_params_}')\n",
    "print(f'meilleur estimateur :\\n{clfGridSearchCV.best_estimator_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a gridsearch taking the previous parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'SVM' : SVC()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'RandomForestClassifier' : [\n",
    "        {'n_estimators' : [4, 6, 9]}, \n",
    "        {'max_features' : ['log2', 'sqrt', 'auto']}, \n",
    "        {'criterion' : ['entropy', 'gini']}, \n",
    "        {'max_depth' : [2, 3, 5, 10]}, \n",
    "        {'min_samples_split' : [2, 3, 5]}, \n",
    "        {'min_samples_leaf' : [1, 5, 8]}\n",
    "    ], \n",
    "    'DecisionTreeClassifier' : [\n",
    "        {'max_depth' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, \n",
    "        {'criterion' : ['gini', 'entropy']}, \n",
    "        {'min_samples_leaf' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "    ],\n",
    "    'SVM' : [\n",
    "        {'C': [1, 0.001]}, \n",
    "        {'gamma': [0.001]}, \n",
    "        {'kernel': ['linear']}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result:\n",
    "    def __init__(self, name, score, parameters):\n",
    "        self.name = name\n",
    "        self.score = score\n",
    "        self.parameters = parameters\n",
    "    def __repr__(self):\n",
    "        return repr((self.name, self.score, self.parameters))\n",
    "\n",
    "results = []\n",
    "myScoring = 'accuracy'\n",
    "\n",
    "for key, value in classifiers.items():\n",
    "    clfGridSearchCV = GridSearchCV(estimator = value, param_grid = params[key], scoring = myScoring, cv = 10, n_jobs = 1, iid = True)\n",
    "    clfGridSearchCV.fit(X_train1, y_train1)\n",
    "    result = Result(key, clfGridSearchCV.best_score_, clfGridSearchCV.best_estimator_)\n",
    "    results.append(result)\n",
    "\n",
    "results = sorted(results, key = lambda result: result.score, reverse = True)\n",
    "\n",
    "print(f'Le meilleur resultat :')\n",
    "print(f'\\tClassifier : {results[0].name} | score : {results[0].score} | parameters :\\n\\t\\t{results[0].parameters}')\n",
    "\n",
    "print(f'Tous les résultats :')\n",
    "for result in results:\n",
    "    print(f'\\n\\tClassifier : {result.name} | score : {result.score} | parameters :\\n\\t\\t{result.parameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the best learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'modeles/attemp1.sav'\n",
    "pickle.dump(results[0].parameters, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the best model to test it with y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'modeles/attemp1.sav'\n",
    "clfLoaded = pickle.load(open(filename, 'rb'))\n",
    "print(f'Modèle chargé :\\n{clfLoaded}\\n')\n",
    "\n",
    "result = clfLoaded.predict(X_test1)\n",
    "\n",
    "print(f'Accuracy : {accuracy_score(result, y_test1)}\\n')\n",
    "print(f'Matrice de confusion :\\n{confusion_matrix(y_test1, result)}\\n')\n",
    "print(f'Classification report :\\n{classification_report(y_test1, result)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
