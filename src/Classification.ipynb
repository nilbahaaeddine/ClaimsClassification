{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Projet Data Science </h1></div>\n",
    "<div align=\"center\"><h2> Classification d'assertions selon leur valeurs de véracité ( automatic fact-checking ) </h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Imports\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import warnings\n",
    "import nltk\n",
    "import pickle\n",
    "import unicodedata\n",
    "import inflect\n",
    "import re\n",
    "import time\n",
    "\n",
    "from enum import Enum\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Downloads\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Declarations\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedRatings(Enum):\n",
    "    FALSE = 1\n",
    "    MIXTURE = 2\n",
    "    TRUE = 3\n",
    "    OTHER = -1\n",
    "\n",
    "_normalization_dictionary = {  # type: Dict[str, Dict[str,NormalizedRatings]]\n",
    "    \"politifact\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'incorrect': NormalizedRatings.FALSE,\n",
    "        'pants-fire': NormalizedRatings.FALSE,\n",
    "        'pants on fire': NormalizedRatings.FALSE,\n",
    "        'pants on fire!': NormalizedRatings.FALSE,\n",
    "        'false': NormalizedRatings.FALSE,\n",
    "        'mostly correct': NormalizedRatings.MIXTURE,\n",
    "        'mostly false': NormalizedRatings.MIXTURE,\n",
    "        'barely true': NormalizedRatings.MIXTURE,\n",
    "        'half true': NormalizedRatings.MIXTURE,\n",
    "        'half-true': NormalizedRatings.MIXTURE,\n",
    "        'mostly true': NormalizedRatings.MIXTURE,\n",
    "        'true': NormalizedRatings.TRUE,\n",
    "        'correct': NormalizedRatings.TRUE\n",
    "    },\n",
    "    \"snopes\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'false': NormalizedRatings.FALSE,\n",
    "        'legend': NormalizedRatings.FALSE,\n",
    "        'mixture': NormalizedRatings.MIXTURE,\n",
    "        'mixture:': NormalizedRatings.MIXTURE,\n",
    "        'true': NormalizedRatings.TRUE,\n",
    "        'mostly false': NormalizedRatings.MIXTURE,\n",
    "        'mostly true': NormalizedRatings.MIXTURE,\n",
    "        'partly true': NormalizedRatings.MIXTURE,\n",
    "        'MIXTURE OF TRUE AND FALSE INFORMATION': NormalizedRatings.MIXTURE,\n",
    "        'MIXTURE OF TRUE AND FALSE INFORMATION:': NormalizedRatings.MIXTURE,\n",
    "        'MIXTURE OF ACCURATE AND  INACCURATE INFORMATION': NormalizedRatings.MIXTURE\n",
    "    },\n",
    "    \"africacheck\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'incorrect': NormalizedRatings.FALSE,\n",
    "        'mostly-correct': NormalizedRatings.MIXTURE,\n",
    "        'correct': NormalizedRatings.TRUE\n",
    "    },\n",
    "    \"factscan\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'false': NormalizedRatings.FALSE,\n",
    "        'true': NormalizedRatings.TRUE,\n",
    "        'Misleading': NormalizedRatings.OTHER\n",
    "    },\n",
    "    \"truthorfiction\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'fiction': NormalizedRatings.FALSE,\n",
    "        'truth': NormalizedRatings.TRUE,\n",
    "        'truth & fiction': NormalizedRatings.MIXTURE,\n",
    "        'mostly fiction': NormalizedRatings.MIXTURE,\n",
    "        'truth & misleading': NormalizedRatings.MIXTURE,\n",
    "        'mostly truth': NormalizedRatings.MIXTURE\n",
    "    },\n",
    "    \"checkyourfact\": {  # type: Dict[str,NormalizedRatings]\n",
    "        'False': NormalizedRatings.FALSE,\n",
    "        'True': NormalizedRatings.TRUE,\n",
    "        'Mostly True': NormalizedRatings.MIXTURE,\n",
    "        'true/false': NormalizedRatings.MIXTURE,\n",
    "        'truth & misleading': NormalizedRatings.MIXTURE,\n",
    "        'mostly truth': NormalizedRatings.MIXTURE,\n",
    "        'misleading': NormalizedRatings.MIXTURE\n",
    "    },\n",
    "    \"factcheck_aap\": {\n",
    "        \"True\": NormalizedRatings.TRUE,\n",
    "        \"False\": NormalizedRatings.FALSE,\n",
    "        \"Mostly True\": NormalizedRatings.MIXTURE,\n",
    "        \"Mostly False\": NormalizedRatings.MIXTURE,\n",
    "        \"Somewhat True\": NormalizedRatings.MIXTURE,\n",
    "        \"Somewhat False\": NormalizedRatings.MIXTURE\n",
    "    },\n",
    "    \"factuel_afp_fr\": {\n",
    "        'Faux': NormalizedRatings.FALSE,\n",
    "        'Totalement faux': NormalizedRatings.FALSE,\n",
    "        'Démenti': NormalizedRatings.FALSE,\n",
    "        \"C'est une oeuvre de fiction\": NormalizedRatings.FALSE,\n",
    "        'Vrai': NormalizedRatings.TRUE,\n",
    "        'Totalement Vrai': NormalizedRatings.TRUE,\n",
    "        'Plutôt vrai': NormalizedRatings.MIXTURE,\n",
    "        'Trompeur': NormalizedRatings.MIXTURE,\n",
    "        'trompeur': NormalizedRatings.MIXTURE,\n",
    "        'Plutôt faux': NormalizedRatings.MIXTURE,\n",
    "        'Presque': NormalizedRatings.MIXTURE,\n",
    "        'Mélangé': NormalizedRatings.MIXTURE,\n",
    "        'Mélange': NormalizedRatings.MIXTURE,\n",
    "        'Inexact': NormalizedRatings.MIXTURE,\n",
    "        'Incertain': NormalizedRatings.MIXTURE,\n",
    "        'Imprécis': NormalizedRatings.MIXTURE,\n",
    "        'Exagéré': NormalizedRatings.MIXTURE,\n",
    "        'Douteux': NormalizedRatings.MIXTURE,\n",
    "    },\n",
    "    \"factcheck_afp\": {\n",
    "        'False': NormalizedRatings.FALSE,\n",
    "        'Fake': NormalizedRatings.FALSE,\n",
    "        'Mixed': NormalizedRatings.MIXTURE,\n",
    "        'Hoax': NormalizedRatings.FALSE,\n",
    "        'Falso': NormalizedRatings.FALSE,\n",
    "        'APRIL FOOL': NormalizedRatings.FALSE\n",
    "    },\n",
    "    \"fullfact\": {\n",
    "        'Correct': NormalizedRatings.TRUE,\n",
    "        'Incorrect': NormalizedRatings.FALSE,\n",
    "        'Not quite': NormalizedRatings.MIXTURE\n",
    "    }\n",
    "}\n",
    "\n",
    "def _standardize_name(original_name: str):\n",
    "    return original_name.strip().lower().replace(\"!\", \"\").replace(\":\", \"\").replace(\"-\", \" \")\n",
    "\n",
    "def normalize(source_name, original_name) -> NormalizedRatings:\n",
    "    '''\n",
    "    Generate a normalized rating from the original ratings on each respective site\n",
    "    :param original_name:\n",
    "    :return normalized_rating: NormalizedRating\n",
    "    '''\n",
    "    try:\n",
    "        source = _normalization_dictionary[source_name]\n",
    "        normalized_value = source[_standardize_name(original_name)]\n",
    "    except KeyError:\n",
    "        normalized_value = NormalizedRatings.OTHER\n",
    "    return normalized_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word) \n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens=normalize(tokens)\n",
    "    text=\"\".join([\" \"+i for i in tokens]).strip()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Loading the dataset\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = [\"ID\", \"ClaimReviewAuthor\", \"ClaimReviewAuthorName\", \"ClaimReviewAuthorURL\",\n",
    "          \"ClaimReviewClaimReviewed\", \"ClaimReviewDatePublished\", \"ClaimReviewSource\", \"ClaimReviewURL\",\n",
    "          \"CreativeWorkAuthorName\", \"CreativeWorkAuthorSameAs\", \"CreativeWorkDatePublished\", \"ExtraBody\",\n",
    "          \"ExtraEntitiesAuthor\", \"ExtraEntitiesBody\", \"ExtraEntitiesClaimReviewClaimReviewed\", \"ExtraEntitiesKeywords\",\n",
    "          \"ExtraReferedLinks\", \"ExtraTags\", \"ExtraTitle\", \"RatingAlternateName\",\n",
    "          \"RatingBestRating\", \"RatingRatingValue\", \"RatingWorstRating\"]\n",
    "\n",
    "labelsClaimsKG = [\"ID\", \"Text\", \"Date\", \"TruthRating\", \"RatingName\", \"Author\", \"Headline\",\n",
    "                  \"NamedEntitiesClaim\", \"NamedEntitiesArticle\", \"Keywords\", \"Source\", \"SourceURL\", \"Link\", \"Language\"]\n",
    "\n",
    "df = pd.read_csv('datasets/ClaimsKG.csv', sep = ',', names = labelsClaimsKG, skiprows = 1, nrows = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Printing informations\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape :\\n{df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Informations :')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Description :')\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing some lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Printing some lines :')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage d'informations sur toutes les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "      print(f'Nombre de valeurs nulles pour {column} :\\n{df[column].isnull().value_counts()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage des colonnes vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = []\n",
    "for column in df.columns:\n",
    "    value = False\n",
    "    value = df[column].isnull().any()\n",
    "    if value:\n",
    "        array.append(column)\n",
    "print(f'Nombre de colonnes vides : {len(array)}\\nLes colonnes vide sont :\\n{array}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description de toutes les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    display(df[column].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a series of unique values in each column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    uniqueValues = df[column].unique()\n",
    "    print(f'Number of unique elements in column {column} : {len(uniqueValues)}, values & type :\\n{uniqueValues}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage du nombre des différents TruthRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'La colonne TruthRating contient :')\n",
    "print(f'{df[df[\"TruthRating\"]==-1][\"ID\"].count()} Other')\n",
    "print(f'{df[df[\"TruthRating\"]==1][\"ID\"].count()} False')\n",
    "print(f'{df[df[\"TruthRating\"]==2][\"ID\"].count()} Mixture')\n",
    "print(f'{df[df[\"TruthRating\"]==3][\"ID\"].count()} True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Visualization\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.countplot(x = 'Source', data = df)\n",
    "plt.setp(chart.get_xticklabels(), rotation = 45, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot(x = 'Source', col = 'RatingName', kind = 'count', data = df)\n",
    "for ax in chart.axes.ravel():\n",
    "    plt.setp(ax.get_xticklabels(), rotation = 45, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot('Source', data = df, hue = 'RatingName', kind = 'count')\n",
    "for ax in chart.axes.ravel():\n",
    "    plt.setp(ax.get_xticklabels(), rotation = 45, horizontalalignment = 'right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), cbar = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Pre-processing\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        General pre-processing\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['ID', 'Date', 'TruthRating', 'SourceURL', 'Link', 'Language'], axis = 1)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unnecessary rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting claims with OTHER RatingName\n",
    "df = df[df.RatingName != 'OTHER']\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing \"Unknown\" & NaN by \"Inconnue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    df[column].replace(to_replace = 'Unknown', value = 'Inconnue', inplace = True)\n",
    "    df[column].replace(np.NaN, 'Inconnue', inplace = True)\n",
    "    \n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if(column != 'RatingName'):\n",
    "        df[column] = df[column].apply(lambda x: clean_text(x))\n",
    "\n",
    "display(df.head())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        TRUE vs FALSE\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTvsF = df.copy()\n",
    "\n",
    "# Suppression de MIXTURE\n",
    "dfTvsF = dfTvsF[dfTvsF.RatingName != 'MIXTURE']\n",
    "\n",
    "# Splitting the datafram\n",
    "dfTvsFHeadline = dfTvsF['Headline']\n",
    "dfTvsFText = dfTvsF['Text']\n",
    "dfTvsFRatingName = dfTvsF['RatingName']\n",
    "dfTvsFAuthor = dfTvsF['Author']\n",
    "dfTvsFNamedEntitiesClaim = dfTvsF['NamedEntitiesClaim']\n",
    "dfTvsFNamedEntitiesArticle = dfTvsF['NamedEntitiesArticle']\n",
    "dfTvsFKeywords = dfTvsF['Keywords']\n",
    "dfTvsFSource = dfTvsF['Source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        First attemp\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTvsFHeadline1 = dfTvsFHeadline.copy()\n",
    "dfTvsFText1 = dfTvsFText.copy()\n",
    "dfTvsFAuthor1 = dfTvsFAuthor.copy()\n",
    "dfTvsFNamedEntitiesClaim1 = dfTvsFNamedEntitiesClaim.copy()\n",
    "dfTvsFNamedEntitiesArticle1 = dfTvsFNamedEntitiesArticle.copy()\n",
    "dfTvsFKeywords1 = dfTvsFKeywords.copy()\n",
    "dfTvsFSource1 = dfTvsFSource.copy()\n",
    "dfTvsFRatingName1 = dfTvsFRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classLabelEncoder = LabelEncoder()\n",
    "\n",
    "dfTvsFHeadline1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFHeadline1))\n",
    "dfTvsFText1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFText1))\n",
    "dfTvsFAuthor1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFAuthor1))\n",
    "dfTvsFNamedEntitiesClaim1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFNamedEntitiesClaim1))\n",
    "dfTvsFNamedEntitiesArticle1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFNamedEntitiesArticle1))\n",
    "dfTvsFKeywords1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFKeywords1))\n",
    "dfTvsFSource1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFSource1))\n",
    "dfTvsFRatingName1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFRatingName1))\n",
    "\n",
    "#TODO HOW TO PUT NAMES\n",
    "dfTvsF1 = pd.concat([dfTvsFHeadline1, dfTvsFText1, dfTvsFAuthor1, dfTvsFNamedEntitiesClaim1, dfTvsFNamedEntitiesArticle1, dfTvsFKeywords1, dfTvsFSource1, dfTvsFRatingName1], axis = 1)\n",
    "\n",
    "display(dfTvsF1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTvsF1.to_csv('datasets/attempTF1.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        Second attemp\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTvsFHeadline2 = dfTvsFHeadline.copy()\n",
    "dfTvsFText2 = dfTvsFText.copy()\n",
    "dfTvsFAuthor2 = dfTvsFAuthor.copy()\n",
    "dfTvsFNamedEntitiesClaim2 = dfTvsFNamedEntitiesClaim.copy()\n",
    "dfTvsFNamedEntitiesArticle2 = dfTvsFNamedEntitiesArticle.copy()\n",
    "dfTvsFKeywords2 = dfTvsFKeywords.copy()\n",
    "dfTvsFSource2 = dfTvsFSource.copy()\n",
    "dfTvsFRatingName2 = dfTvsFRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classLabelEncoder = LabelEncoder()\n",
    "tfidfVectorizer = TfidfVectorizer()\n",
    "\n",
    "dfTvsFHeadline2 = pd.DataFrame(tfidfVectorizer.fit_transform(dfTvsFHeadline2))\n",
    "dfTvsFText2 = pd.DataFrame(tfidfVectorizer.fit_transform(dfTvsFText2))\n",
    "#dfTvsFAuthor2 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFAuthor2))\n",
    "#dfTvsFNamedEntitiesClaim2 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFNamedEntitiesClaim2))\n",
    "#dfTvsFNamedEntitiesArticle2 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFNamedEntitiesArticle2))\n",
    "#dfTvsFKeywords2 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFKeywords2))\n",
    "#dfTvsFSource2 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFSource2))\n",
    "dfTvsFRatingName2 = pd.DataFrame(classLabelEncoder.fit_transform(dfTvsFRatingName2))\n",
    "\n",
    "#TODO HOW TO PUT NAMES\n",
    "dfTvsF2 = pd.concat([dfTvsF2Headline1, dfTvsF2Text1, dfTvsF2Author1, dfTvsF2NamedEntitiesClaim1, dfTvsF2NamedEntitiesArticle1, dfTvsF2Keywords1, dfTvsF2Source1, dfTvsF2RatingName1], axis = 1)\n",
    "\n",
    "display(dfTvsF2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTvsF2.to_csv('datasets/attempTF2.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        TRUE/FALSE vs MIXTURE\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTFvsM = df.copy()\n",
    "\n",
    "# Splitting the datafram\n",
    "dfTFvsMHeadline = dfTFvsM['Headline']\n",
    "dfTFvsMText = dfTFvsM['Text']\n",
    "dfTFvsMRatingName = dfTFvsM['RatingName']\n",
    "dfTFvsMAuthor = dfTFvsM['Author']\n",
    "dfTFvsMNamedEntitiesClaim = dfTFvsM['NamedEntitiesClaim']\n",
    "dfTFvsMNamedEntitiesArticle = dfTFvsM['NamedEntitiesArticle']\n",
    "dfTFvsMKeywords = dfTFvsM['Keywords']\n",
    "dfTFvsMSource = dfTFvsM['Source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>\n",
    "        First attemp\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTFvsMHeadline1 = dfTFvsMHeadline.copy()\n",
    "dfTFvsMText1 = dfTFvsMText.copy()\n",
    "dfTFvsMAuthor1 = dfTFvsMAuthor.copy()\n",
    "dfTFvsMNamedEntitiesClaim1 = dfTFvsMNamedEntitiesClaim.copy()\n",
    "dfTFvsMNamedEntitiesArticle1 = dfTFvsMNamedEntitiesArticle.copy()\n",
    "dfTFvsMKeywords1 = dfTFvsMKeywords.copy()\n",
    "dfTFvsMSource1 = dfTFvsMSource.copy()\n",
    "dfTFvsMRatingName1 = dfTFvsMRatingName.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classLabelEncoder = LabelEncoder()\n",
    "\n",
    "dfTFvsMHeadline1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTFvsMHeadline1))\n",
    "dfTFvsMText1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTFvsMText1))\n",
    "dfTFvsMAuthor1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTFvsMAuthor1))\n",
    "dfTFvsMNamedEntitiesClaim1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTFvsMNamedEntitiesClaim1))\n",
    "dfTFvsMNamedEntitiesArticle1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTFvsMNamedEntitiesArticle1))\n",
    "dfTFvsMKeywords1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTFvsMKeywords1))\n",
    "dfTFvsMSource1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTFvsMSource1))\n",
    "dfTFvsMRatingName1 = pd.DataFrame(classLabelEncoder.fit_transform(dfTFvsMRatingName1))\n",
    "\n",
    "#TODO HOW TO PUT NAMES\n",
    "dfTFvsM1 = pd.concat([dfTFvsMHeadline1, dfTFvsMText1, dfTFvsMAuthor1, dfTFvsMNamedEntitiesClaim1, dfTFvsMNamedEntitiesArticle1, dfTFvsMKeywords1, dfTFvsMSource1, dfTFvsMRatingName1], axis = 1)\n",
    "\n",
    "display(dfTFvsM1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTFvsM1.to_csv('datasets/attempTFM1.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Classification\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        Preparing attemp 1 data for classification\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the transformed data for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClassification1 = pd.read_csv('datasets/attemp1.csv', sep = ';')\n",
    "display(dfClassification1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the learning variables and the variable to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = dfClassification1.values\n",
    "X1 = array1[:,0:7]\n",
    "y1 = array1[:,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut the data set into a test set and a learning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTrainSize = 0.3 # 30% du jeu de données pour le test\n",
    "myTestSize = 1 - myTrainSize # 70% du jeu de données pour l'entraînement\n",
    "seed = 30\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, train_size = myTrainSize, random_state = seed, test_size = myTestSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        Testing the first classifier on attemp 1\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaussianNB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfGaussianNB = GaussianNB()\n",
    "\n",
    "clfGaussianNB.fit(X_train1, y_train1)\n",
    "\n",
    "resultGaussianNB = clfGaussianNB.predict(X_test1)\n",
    "\n",
    "print(f'accuracy : {accuracy_score(resultGaussianNB, y_test1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the confusion matrix and the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Matrice de confusion :\\n{confusion_matrix(y_test1, resultGaussianNB)}')\n",
    "print (f'Classification report :\\n{classification_report(y_test1, resultGaussianNB)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validate with 10 splits (Kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "myKFold = KFold(n_splits = 10, shuffle = True, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the GaussianNB classifier and give the different accuracy for the 10 evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfGaussianNB = GaussianNB()\n",
    "\n",
    "myScoring = 'accuracy'\n",
    "\n",
    "score = cross_val_score(clfGaussianNB, X1, y1, cv = myKFold, scoring = myScoring)\n",
    "\n",
    "print(f'Les différentes accuracy pour les 10 évaluations sont :\\n{score}')\n",
    "print(f'Accuracy moyenne : {score.mean()} | Standard deviation : {score.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>\n",
    "        Testing several classifiers\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma = 'auto')))\n",
    "models.append(('RFO', RandomForestClassifier()))\n",
    "#models.append(('LR', LogisticRegression()))\n",
    "#models.append(('LSVC', LinearSVC(max_iter = 3000)))\n",
    "#models.append(('DTR', DecisionTreeRegressor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "myScoring = 'accuracy'\n",
    "scores = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    myKFold = KFold(n_splits = 10, random_state = seed)\n",
    "    startTime = time.time()\n",
    "    score = cross_val_score(model, X1, y1, cv = myKFold, scoring = myScoring)\n",
    "    endTime = time.time()\n",
    "    scores.append(score)\n",
    "    names.append(name)\n",
    "    print(f'{name}\\t(Time : {endTime - startTime}\\t| {score.mean()}\\t| {score.std()})\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying results of the different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Comparaison des algorithmes')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(scores)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    myKFold = KFold(n_splits = 10, shuffle = True, random_state = seed)\n",
    "    score = cross_val_score(model, X1, y1, cv = myKFold, scoring = myScoring)\n",
    "    scores.append(score)\n",
    "    names.append(name)\n",
    "    print(f'{name} : {score.mean()} | {score.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying results of the different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Comparaison des algorithmes')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(scores)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply GridSearchCV to RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridParam = {'n_estimators': [4, 6, 9], \n",
    "             'max_features': ['log2', 'sqrt','auto'], \n",
    "             'criterion': ['entropy', 'gini'], \n",
    "             'max_depth': [2, 3, 5, 10], \n",
    "             'min_samples_split': [2, 3, 5], \n",
    "             'min_samples_leaf': [1, 5, 8]\n",
    "            }\n",
    "\n",
    "myScoring = 'accuracy'\n",
    "\n",
    "clfGridSearchCV = GridSearchCV(estimator = RandomForestClassifier(), param_grid = gridParam, scoring = myScoring, cv = 5, n_jobs = -1, iid = True, return_train_score = True)\n",
    "\n",
    "clfGridSearchCV.fit(X_train1, y_train1)\n",
    "\n",
    "print(f'meilleur score : {clfGridSearchCV.best_score_}')\n",
    "print(f'meilleurs paramètres :\\n{clfGridSearchCV.best_params_}')\n",
    "print(f'meilleur estimateur :\\n{clfGridSearchCV.best_estimator_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply GridSearchCV to DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridParam = {'max_depth' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "             'criterion' : ['gini', 'entropy'], \n",
    "             'min_samples_leaf' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "            }\n",
    "\n",
    "myScoring = 'accuracy'\n",
    "\n",
    "clfGridSearchCV = GridSearchCV(estimator = DecisionTreeClassifier(), param_grid = gridParam, scoring = myScoring, cv = 10, n_jobs = -1, iid = True, return_train_score = True)\n",
    "\n",
    "clfGridSearchCV.fit(X_train1, y_train1)  \n",
    "\n",
    "print(f'meilleur score : {clfGridSearchCV.best_score_}')\n",
    "print(f'meilleurs paramètres :\\n{clfGridSearchCV.best_params_}')\n",
    "print(f'meilleur estimateur :\\n{clfGridSearchCV.best_estimator_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply GridSearchCV to SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridParam = {'C' : [0.001, 0.01, 0.1, 1, 10], \n",
    "             'gamma' : [0.001, 0.01, 0.1, 1], \n",
    "             'kernel' : ['linear', 'rbf']\n",
    "            }\n",
    "\n",
    "myScoring = 'accuracy'\n",
    "\n",
    "clfGridSearchCV = GridSearchCV(estimator = SVC(), param_grid = gridParam, scoring = myScoring, cv = 5, n_jobs = 1, iid = True, return_train_score = True)\n",
    "\n",
    "clfGridSearchCV.fit(X_train1, y_train1)\n",
    "\n",
    "print(f'meilleur score : {clfGridSearchCV.best_score_}')\n",
    "print(f'meilleurs paramètres :\\n{clfGridSearchCV.best_params_}')\n",
    "print(f'meilleur estimateur :\\n{clfGridSearchCV.best_estimator_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a gridsearch taking the previous parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'SVM' : SVC()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'RandomForestClassifier' : [\n",
    "        {'n_estimators' : [4, 6, 9]}, \n",
    "        {'max_features' : ['log2', 'sqrt', 'auto']}, \n",
    "        {'criterion' : ['entropy', 'gini']}, \n",
    "        {'max_depth' : [2, 3, 5, 10]}, \n",
    "        {'min_samples_split' : [2, 3, 5]}, \n",
    "        {'min_samples_leaf' : [1, 5, 8]}\n",
    "    ], \n",
    "    'DecisionTreeClassifier' : [\n",
    "        {'max_depth' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, \n",
    "        {'criterion' : ['gini', 'entropy']}, \n",
    "        {'min_samples_leaf' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "    ],\n",
    "    'SVM' : [\n",
    "        {'C': [0.001]}, \n",
    "        {'gamma': [0.001]}, \n",
    "        {'kernel': ['linear']}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result:\n",
    "    def __init__(self, name, score, parameters):\n",
    "        self.name = name\n",
    "        self.score = score\n",
    "        self.parameters = parameters\n",
    "    def __repr__(self):\n",
    "        return repr((self.name, self.score, self.parameters))\n",
    "\n",
    "results = []\n",
    "myScoring = 'accuracy'\n",
    "\n",
    "for key, value in classifiers.items():\n",
    "    clfGridSearchCV = GridSearchCV(estimator = value, param_grid = params[key], scoring = myScoring, cv = 10, n_jobs = 1, iid = True)\n",
    "    clfGridSearchCV.fit(X_train1, y_train1)\n",
    "    result = Result(key, clfGridSearchCV.best_score_, clfGridSearchCV.best_estimator_)\n",
    "    results.append(result)\n",
    "\n",
    "results = sorted(results, key = lambda result: result.score, reverse = True)\n",
    "\n",
    "print(f'Le meilleur resultat :')\n",
    "print(f'\\tClassifier : {results[0].name} | score : {results[0].score} | parameters :\\n\\t\\t{results[0].parameters}')\n",
    "\n",
    "print(f'Tous les résultats :')\n",
    "for result in results:\n",
    "    print(f'\\n\\tClassifier : {result.name} | score : {result.score} | parameters :\\n\\t\\t{result.parameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the best learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'modeles/attemp1.sav'\n",
    "pickle.dump(results[0].parameters, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the best model to test it with y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'modeles/attemp1.sav'\n",
    "clfLoaded = pickle.load(open(filename, 'rb'))\n",
    "print(f'Modèle chargé :\\n{clfLoaded}\\n')\n",
    "\n",
    "result = clfLoaded.predict(X_test1)\n",
    "\n",
    "print(f'Accuracy : {accuracy_score(result, y_test1)}\\n')\n",
    "print(f'Matrice de confusion :\\n{confusion_matrix(y_test1, result)}\\n')\n",
    "print(f'Classification report :\\n{classification_report(y_test1, result)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
